# CoLESä¸‹æ¸¸å¾®è°ƒæ¨¡å‹ä½¿ç”¨æŒ‡å—

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•ä½¿ç”¨é¢„è®­ç»ƒçš„CoLESæ¨¡å‹è¿›è¡Œä¸‹æ¸¸ä»»åŠ¡çš„å¾®è°ƒè®­ç»ƒã€‚

## ğŸ“ æ–‡ä»¶ç»“æ„

```
â”œâ”€â”€ downstream_finetune_model.py    # å¾®è°ƒæ¨¡å‹æ ¸å¿ƒå®ç°
â”œâ”€â”€ finetune_example.py            # å®Œæ•´çš„å¾®è°ƒè®­ç»ƒç¤ºä¾‹
â”œâ”€â”€ simple_finetune_test.py        # ç®€åŒ–çš„åŠŸèƒ½æµ‹è¯•è„šæœ¬
â””â”€â”€ README_FinetuneModel.md        # æœ¬æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºæœ¬ä½¿ç”¨

```python
from downstream_finetune_model import CoLESFinetuneModule, create_finetune_model
from ptls.nn import TransformerSeqEncoder

# åˆ›å»ºæˆ–åŠ è½½é¢„è®­ç»ƒç¼–ç å™¨
seq_encoder = load_your_pretrained_encoder()

# åˆ›å»ºå¾®è°ƒæ¨¡å‹
model = create_finetune_model(
    seq_encoder=seq_encoder,
    n_classes=2,                    # äºŒåˆ†ç±»
    head_hidden_dims=[256, 128],    # MLPå¤´éšè—å±‚ç»´åº¦
    dropout=0.3,
    lr=1e-4,
    freeze_epochs=5                 # å‰5ä¸ªepochå†»ç»“ç¼–ç å™¨
)

# ä½¿ç”¨PyTorch Lightningè¿›è¡Œè®­ç»ƒ
trainer = pl.Trainer(max_epochs=50)
trainer.fit(model, train_dataloader, val_dataloader)
```

### 2. è¿è¡Œæµ‹è¯•

```bash
# è¿è¡ŒåŠŸèƒ½æµ‹è¯•
python simple_finetune_test.py

# è¿è¡Œå®Œæ•´è®­ç»ƒç¤ºä¾‹
python finetune_example.py
```

## ğŸ—ï¸ æ¨¡å‹æ¶æ„

### æ•´ä½“ç»“æ„

```
è¾“å…¥æ•°æ® (PaddedBatch)
    â†“
é¢„è®­ç»ƒç¼–ç å™¨ (Backbone)
    â†“
ç‰¹å¾å‘é‡ [B, D]
    â†“
MLPåˆ†ç±»å¤´ (Head)
    â†“
åˆ†ç±»logits [B, n_classes]
```

### æ ¸å¿ƒç»„ä»¶

1. **ä¸»å¹²ç½‘ç»œ (Backbone)**
   - ä½¿ç”¨é¢„è®­ç»ƒçš„PTLSç¼–ç å™¨ï¼ˆå¦‚TransformerSeqEncoderï¼‰
   - æ”¯æŒå†»ç»“/è§£å†»å‚æ•°
   - è¾“å‡ºå›ºå®šç»´åº¦çš„ç‰¹å¾å‘é‡

2. **åˆ†ç±»å¤´ (Head)**
   - å¤šå±‚MLPç»“æ„
   - æ”¯æŒè‡ªå®šä¹‰éšè—å±‚ç»´åº¦
   - åŒ…å«Dropoutå’Œæ¿€æ´»å‡½æ•°

3. **è®­ç»ƒç­–ç•¥**
   - æ”¯æŒæ¸è¿›å¼è§£å†»ï¼ˆå…ˆå†»ç»“backboneå‡ ä¸ªepochï¼‰
   - å¤šç§ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
   - ç±»åˆ«æƒé‡å’Œæ ‡ç­¾å¹³æ»‘

## âš™ï¸ é…ç½®å‚æ•°

### CoLESFinetuneModule å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `seq_encoder` | nn.Module | - | é¢„è®­ç»ƒçš„åºåˆ—ç¼–ç å™¨ |
| `n_classes` | int | 2 | åˆ†ç±»ç±»åˆ«æ•° |
| `head_hidden_dims` | list | None | MLPå¤´éšè—å±‚ç»´åº¦ |
| `dropout` | float | 0.2 | Dropoutæ¦‚ç‡ |
| `lr` | float | 1e-3 | å­¦ä¹ ç‡ |
| `weight_decay` | float | 1e-5 | æƒé‡è¡°å‡ |
| `freeze_encoder` | bool | False | æ˜¯å¦åˆå§‹å†»ç»“ç¼–ç å™¨ |
| `freeze_epochs` | int | 0 | å†»ç»“ç¼–ç å™¨çš„epochæ•° |
| `optimizer_type` | str | 'adam' | ä¼˜åŒ–å™¨ç±»å‹ ('adam', 'adamw') |
| `scheduler_type` | str | None | è°ƒåº¦å™¨ç±»å‹ ('cosine', 'plateau', None) |
| `class_weights` | Tensor | None | ç±»åˆ«æƒé‡ |
| `label_smoothing` | float | 0.0 | æ ‡ç­¾å¹³æ»‘å‚æ•° |

### MLPHead å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `input_dim` | int | - | è¾“å…¥ç‰¹å¾ç»´åº¦ |
| `hidden_dims` | list | None | éšè—å±‚ç»´åº¦åˆ—è¡¨ |
| `n_classes` | int | 2 | è¾“å‡ºç±»åˆ«æ•° |
| `dropout` | float | 0.2 | Dropoutæ¦‚ç‡ |
| `activation` | str | 'relu' | æ¿€æ´»å‡½æ•°ç±»å‹ |

## ğŸ”§ é«˜çº§åŠŸèƒ½

### 1. æ¸è¿›å¼è§£å†»

```python
model = CoLESFinetuneModule(
    seq_encoder=encoder,
    freeze_epochs=5  # å‰5ä¸ªepochå†»ç»“ç¼–ç å™¨
)
```

### 2. ç±»åˆ«æƒé‡å¤„ç†

```python
# å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
class_weights = torch.tensor([0.3, 0.7])  # ç±»åˆ«0æƒé‡å°ï¼Œç±»åˆ«1æƒé‡å¤§
model = CoLESFinetuneModule(
    seq_encoder=encoder,
    class_weights=class_weights
)
```

### 3. å­¦ä¹ ç‡è°ƒåº¦

```python
model = CoLESFinetuneModule(
    seq_encoder=encoder,
    optimizer_type='adamw',
    scheduler_type='cosine',  # ä½™å¼¦é€€ç«
    lr=1e-4
)
```

### 4. åŠ è½½é¢„è®­ç»ƒæƒé‡

```python
model = CoLESFinetuneModule(seq_encoder=encoder)
model.load_pretrained_encoder(
    checkpoint_path='./pretrained_model.ckpt',
    strict=False  # å…è®¸éƒ¨åˆ†åŒ¹é…
)
```

### 5. è·å–ç‰¹å¾åµŒå…¥

```python
# è·å–æ•°æ®çš„åµŒå…¥è¡¨ç¤º
embeddings, labels = model.get_embeddings(dataloader)
print(f"åµŒå…¥ç»´åº¦: {embeddings.shape}")
```

## ğŸ“Š è®­ç»ƒç›‘æ§

### è‡ªåŠ¨è®°å½•çš„æŒ‡æ ‡

- **è®­ç»ƒæŒ‡æ ‡**: `train_loss`, `train_acc`
- **éªŒè¯æŒ‡æ ‡**: `val_loss`, `val_acc`, `val_f1`
- **äºŒåˆ†ç±»é¢å¤–æŒ‡æ ‡**: `val_auc`

### ä½¿ç”¨TensorBoard

```python
from pytorch_lightning.loggers import TensorBoardLogger

logger = TensorBoardLogger(save_dir='./logs', name='finetune')
trainer = pl.Trainer(logger=logger)
```

### æ¨¡å‹æ£€æŸ¥ç‚¹

```python
from pytorch_lightning.callbacks import ModelCheckpoint

checkpoint_callback = ModelCheckpoint(
    monitor='val_acc',
    mode='max',
    save_top_k=3,
    filename='best_model_{epoch:02d}_{val_acc:.3f}'
)
```

## ğŸ¯ æœ€ä½³å®è·µ

### 1. æ•°æ®å‡†å¤‡
- ç¡®ä¿è¾“å…¥æ•°æ®æ ¼å¼ä¸ºPaddedBatch
- æ ‡ç­¾åº”ä¸ºLongTensorç±»å‹
- åˆç†è®¾ç½®batch_sizeï¼ˆæ¨è16-64ï¼‰

### 2. è®­ç»ƒç­–ç•¥
- ä½¿ç”¨æ¸è¿›å¼è§£å†»ï¼šå…ˆå†»ç»“backbone 3-10ä¸ªepoch
- è®¾ç½®è¾ƒå°çš„å­¦ä¹ ç‡ï¼ˆ1e-4åˆ°1e-5ï¼‰
- ä½¿ç”¨æƒé‡è¡°å‡é˜²æ­¢è¿‡æ‹Ÿåˆ
- å¯ç”¨æ¢¯åº¦è£å‰ªï¼ˆgradient_clip_val=1.0ï¼‰

### 3. è¶…å‚æ•°è°ƒä¼˜
```python
# æ¨èçš„è¶…å‚æ•°ç»„åˆ
config = {
    'lr': 1e-4,
    'weight_decay': 1e-5,
    'dropout': 0.3,
    'freeze_epochs': 5,
    'head_hidden_dims': [256, 128],
    'optimizer_type': 'adamw',
    'scheduler_type': 'cosine'
}
```

### 4. æ€§èƒ½ä¼˜åŒ–
- ä½¿ç”¨æ··åˆç²¾åº¦è®­ç»ƒï¼š`precision='16-mixed'`
- å¯ç”¨pin_memoryï¼š`pin_memory=True`
- åˆç†è®¾ç½®num_workers

## ğŸ› å¸¸è§é—®é¢˜

### Q1: å¦‚ä½•å¤„ç†ç»´åº¦ä¸åŒ¹é…ï¼Ÿ
```python
# æ£€æŸ¥ç¼–ç å™¨è¾“å‡ºç»´åº¦
print(f"ç¼–ç å™¨è¾“å‡ºç»´åº¦: {seq_encoder.embedding_size}")

# ç¡®ä¿MLPå¤´è¾“å…¥ç»´åº¦æ­£ç¡®
model = CoLESFinetuneModule(
    seq_encoder=seq_encoder,
    head_hidden_dims=[seq_encoder.embedding_size // 2, 64]
)
```

### Q2: å¦‚ä½•å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ï¼Ÿ
```python
# è®¡ç®—ç±»åˆ«æƒé‡
from sklearn.utils.class_weight import compute_class_weight

class_weights = compute_class_weight(
    'balanced', 
    classes=np.unique(labels), 
    y=labels
)
class_weights = torch.tensor(class_weights, dtype=torch.float)
```

### Q3: è®­ç»ƒè¿‡ç¨‹ä¸­lossä¸ä¸‹é™ï¼Ÿ
- æ£€æŸ¥å­¦ä¹ ç‡æ˜¯å¦è¿‡å¤§æˆ–è¿‡å°
- ç¡®è®¤æ•°æ®é¢„å¤„ç†æ˜¯å¦æ­£ç¡®
- å°è¯•è°ƒæ•´freeze_epochs
- æ£€æŸ¥æ¢¯åº¦æ˜¯å¦æ­£å¸¸ä¼ æ’­

### Q4: å¦‚ä½•ä¿å­˜å’ŒåŠ è½½æ¨¡å‹ï¼Ÿ
```python
# ä¿å­˜
trainer.save_checkpoint('model.ckpt')

# åŠ è½½
model = CoLESFinetuneModule.load_from_checkpoint(
    'model.ckpt',
    seq_encoder=seq_encoder  # éœ€è¦é‡æ–°æä¾›ç¼–ç å™¨
)
```

## ğŸ“ˆ æ€§èƒ½åŸºå‡†

### å…¸å‹æ€§èƒ½æŒ‡æ ‡
- **æ”¶æ•›é€Ÿåº¦**: é€šå¸¸åœ¨10-30ä¸ªepochå†…æ”¶æ•›
- **å†…å­˜ä½¿ç”¨**: å–å†³äºbatch_sizeå’Œåºåˆ—é•¿åº¦
- **è®­ç»ƒæ—¶é—´**: ä¸æ•°æ®é›†å¤§å°å’Œæ¨¡å‹å¤æ‚åº¦ç›¸å…³

### ä¼˜åŒ–å»ºè®®
1. ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å¯æ˜¾è‘—æå‡æ€§èƒ½
2. æ¸è¿›å¼è§£å†»æ¯”ç›´æ¥å¾®è°ƒæ•ˆæœæ›´å¥½
3. é€‚å½“çš„æ­£åˆ™åŒ–å¯é˜²æ­¢è¿‡æ‹Ÿåˆ

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [PTLSå®˜æ–¹æ–‡æ¡£](https://github.com/dllllb/pytorch-lifestream)
- [PyTorch Lightningæ–‡æ¡£](https://pytorch-lightning.readthedocs.io/)
- [ä¸‹æ¸¸æ•°æ®åŠ è½½å™¨æ–‡æ¡£](./README_DownstreamDataLoader.md)

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v1.0.0**: åˆå§‹ç‰ˆæœ¬ï¼Œæ”¯æŒåŸºæœ¬å¾®è°ƒåŠŸèƒ½
- æ”¯æŒå¤šç§ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
- æ”¯æŒæ¸è¿›å¼è§£å†»å’Œç±»åˆ«æƒé‡
- å®Œæ•´çš„è®­ç»ƒç›‘æ§å’Œæ—¥å¿—è®°å½•